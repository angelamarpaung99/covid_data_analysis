# -*- coding: utf-8 -*-
"""predict-corona.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1syAkv_d-y_Cki5pzIt-0JtVy9QpIAlNz
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')

# Get data from Github
import numpy as np
from math import sqrt
from sklearn.metrics import mean_squared_error
import pandas as pd

# confirmed cases
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv'
confirmed = pd.read_csv(url, error_bad_lines=False)

# death cases
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv'
death = pd.read_csv(url, error_bad_lines=False)

# recovered cases
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv'
recover = pd.read_csv(url, error_bad_lines=False)

# fix region names
confirmed['Country/Region']= confirmed['Country/Region'].str.replace("Mainland China", "China")
confirmed['Country/Region']= confirmed['Country/Region'].str.replace("US", "United States")
confirmed['Country/Region']= confirmed['Country/Region'].str.replace("UK", "United Kingdom")
death['Country/Region']= death['Country/Region'].str.replace("Mainland China", "China")
death['Country/Region']= death['Country/Region'].str.replace("US", "United States")
death['Country/Region']= death['Country/Region'].str.replace("UK", "United Kingdom")
recover['Country/Region']= recover['Country/Region'].str.replace("Mainland China", "China")
recover['Country/Region']= recover['Country/Region'].str.replace("US", "United States")
recover['Country/Region']= recover['Country/Region'].str.replace("UK", "United Kingdom")

confirmed.info()

url = 'https://raw.githubusercontent.com/Rank23/COVID19/master/population.csv'
population = pd.read_csv(url, sep=",", encoding='latin1')

population.head()

confirmed=pd.merge(confirmed, population,how='left' ,on=['Province/State','Country/Region'])
death=pd.merge(death, population,how='left' ,on=['Province/State','Country/Region'])
recover=pd.merge(recover, population,how='left' ,on=['Province/State','Country/Region'])

confirmed[' Population '].fillna(1, inplace = True)
death[' Population '].fillna(1, inplace = True)
recover[' Population '].fillna(1, inplace = True)

confirmed[' Population ']

# pip install countryinfo

# !pip install qwikidata

# import qwikidata
# import qwikidata.sparql
# ##################################################### This was already done ####################################################
# def get_city_wikidata(city, country):
#     query = """
#     SELECT ?city ?cityLabel ?country ?countryLabel ?population
#     WHERE
#     {
#       ?city rdfs:label '%s'@en.
#       ?city wdt:P1082 ?population.
#       ?city wdt:P17 ?country.
#       ?city rdfs:label ?cityLabel.
#       ?country rdfs:label ?countryLabel.
#       FILTER(LANG(?cityLabel) = "en").
#       FILTER(LANG(?countryLabel) = "en").
#       FILTER(CONTAINS(?countryLabel, "%s")).
#     }
#     """ % (city, country)

#     res = qwikidata.sparql.return_sparql_query_results(query)
#     out = res['results']['bindings'][0]
#     return out

# from countryinfo import CountryInfo

# # country = CountryInfo('Singapore')
# # country.info()['population']

# # Get the population of the city

# population = []
# for i in range(0,len(confirmed)):
#   try:
#     t=get_city_wikidata(confirmed.iloc[i,0],confirmed.iloc[i,1])
#     t={key: t[key] for key in t.keys()&{'population'}} 
#     t=list(t.values())
#     population.append(int(t[0]['value']))
#   except Exception:
#     try:
#       population.append(int(CountryInfo(confirmed.iloc[i,1]).info()['population']))
#     except Exception:
#       population.append(1)

# merge region
confirmed['region']=confirmed['Country/Region'].map(str)+'_'+confirmed['Province/State'].map(str)
death['region']=death['Country/Region'].map(str)+'_'+death['Province/State'].map(str)
recover['region']=recover['Country/Region'].map(str)+'_'+recover['Province/State'].map(str)
confirmed.iloc[:5,:]

def create_ts(df):
  ts=df
  ts=ts.drop(['Province/State', 'Country/Region','Lat', 'Long',' Population '], axis=1)
  ts.set_index('region')
  ts=ts.T
  ts.columns=ts.loc['region']
  ts=ts.drop('region')
  ts=ts.fillna(0)
  ts=ts.reindex(sorted(ts.columns), axis=1)
  return (ts)

ts=create_ts(confirmed)
ts_d=create_ts(death)
ts_rec=create_ts(recover)

ts

# sort regions with number of confirmed cases largest to smallest
import matplotlib.pyplot as plt
p=ts.reindex(ts.max().sort_values(ascending=False).index, axis=1)

p.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Confirmed - Hubei',fontdict={'fontsize': 22})
p.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Confirmed - Major areas',fontdict={'fontsize': 22})

p_d=ts_d.reindex(ts.mean().sort_values(ascending=False).index, axis=1)
p_d.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Death - Hubei',fontdict={'fontsize': 22})
p_d.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Death - Major areas',fontdict={'fontsize': 22})

p_r=ts_rec.reindex(ts.mean().sort_values(ascending=False).index, axis=1)
p_r.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Recovered - Hubei',fontdict={'fontsize': 22})
p_r.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Recovered - Major areas',fontdict={'fontsize': 22})

# Create data for R script
ts_r=ts.reset_index()
ts_r=ts_r.rename(columns = {'index':'date'})
ts_r['date']=pd.to_datetime(ts_r['date']).dt.date
ts_r.to_csv('/content/drive/My Drive/corona-virus/ts_r.csv')

ts_r.tail()

ts_r.date

ts_r

from pandas import read_csv
from statsmodels.tsa.arima_model import ARIMA
import numpy
df = ts_r.loc[:, ['date', 'China_Hubei']]
df = df.set_index('date')
dataset, validation = df.iloc[0:-1], df.iloc[-1]

df

# make data stationary
def difference(dataset, interval=1):
	diff = list()
	for i in range(interval, len(dataset)):
		value = dataset[i] - dataset[i - interval]
		diff.append(value)
	return numpy.array(diff)

# invert differenced value
def inverse_difference(history, yhat, interval=1):
	return yhat + history[-interval]

# seasonal difference
X = dataset.values
differenced = difference(X, 1)
# fit model
model = ARIMA(differenced, order=(1,1,0))
model_fit = model.fit(disp=0)
# print summary of fit model
print(model_fit.summary())

from pandas import read_csv
from statsmodels.tsa.arima_model import ARIMA
import numpy
df = ts_r.loc[:, ['date', 'China_Hubei']]
df = df.set_index('date')
dataset, validation = df.iloc[0:-5], df.iloc[-5:]

# seasonal difference
start_index = len(differenced)
end_index = start_index + 6
forecast = model_fit.predict(start=start_index, end=end_index)
# invert the differenced forecast to something usable
history = [x for x in X]
day = 1
predict=[]
for yhat in forecast:
	inverted = inverse_difference(history, yhat, 1)
	print('Day %d: %f' % (day, inverted))
	#predict.append(inverted)
	history.append(inverted)
	day += 1

# one-step out of sample forecast
# forecast = model_fit.forecast(steps=7)[0]
# # invert the differenced forecast to something usable
# history = [x for x in X]
# day = 1
# for yhat in forecast:
# 	inverted = inverse_difference(history, yhat, 1)
# 	print('Day %d: %f' % (day, inverted))
# 	history.append(inverted)
# 	day += 1
# print('Forecast: %f' % forecast)

# # one-step out-of sample forecast
# forecast = model_fit.forecast(start=start_index, end=end_index)
# # invert the differenced forecast to something usable
# forecast = inverse_difference(X, forecast, 1)
# print('Forecast: %f' % forecast)

from sklearn.metrics import mean_squared_error
prediction = [67596.570603,67596.522792, 67598.171777,67599.197363, 67600.451984, 67601.622458, 67602.823847]
true = [66907,67103,67217,67332,67466,67592,67666]
mean_squared_error(prediction, true)

len(df)

len(dataset)

a = []
for x in range(len(df)):
  a.append(df.index[x].strftime('%d/%m'))
# print(a)
# print(history)

plt.figure(figsize=(35, 5))
plt.plot(a[:46],history[:46], color='blue',label='Train')
plt.plot(a[-9:],history[-9:],'--b',color='red',label='Predicted',marker='x')

plt.show()

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

result = ts_r['China_Beijing'].iloc[-1]
df = ts_r[:-1].loc[:, ['date', 'China_Beijing']]
df = df.set_index('date')
plt.xlabel('Date')
plt.ylabel('Confirmed Corona in China')
plt.plot(df)

rolling_mean = df.rolling(window = 12).mean()
rolling_std = df.rolling(window = 12).std()
plt.plot(df, color = 'blue', label = 'Original')
plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')
plt.plot(rolling_std, color = 'black', label = 'Rolling Std')
plt.legend(loc = 'best')
plt.title('Rolling Mean & Rolling Standard Deviation')
plt.show()

result = adfuller(df['China_Beijing'])
print('ADF Statistic: {}'.format(result[0]))
print('p-value: {}'.format(result[1])) # p threshold = 0.05
print('Critical Values:')
for key, value in result[4].items():
    print('\t{}: {}'.format(key, value))

df_log = np.log(df)
plt.plot(df_log)

def get_stationarity(timeseries):
    
    # rolling statistics
    rolling_mean = timeseries.rolling(window=12).mean()
    rolling_std = timeseries.rolling(window=12).std()
    
    # rolling statistics plot
    original = plt.plot(timeseries, color='blue', label='Original')
    mean = plt.plot(rolling_mean, color='red', label='Rolling Mean')
    std = plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    
    # Dickey–Fuller test:
    result = adfuller(timeseries['China_Beijing'])
    print('ADF Statistic: {}'.format(result[0]))
    print('p-value: {}'.format(result[1]))
    print('Critical Values:')
    for key, value in result[4].items():
        print('\t{}: {}'.format(key, value))

# Substrating the rolling mean for stationary
rolling_mean = df_log.rolling(window=12).mean()
df_log_minus_mean = df_log - rolling_mean
df_log_minus_mean.dropna(inplace=True)
get_stationarity(df_log_minus_mean)

# Exponencial decay for stationary
rolling_mean_exp_decay = df_log.ewm(halflife=12, min_periods=0, adjust=True).mean()
df_log_exp_decay = df_log - rolling_mean_exp_decay
df_log_exp_decay.dropna(inplace=True)
get_stationarity(df_log_exp_decay)

df_log_shift = df_log - df_log.shift()
df_log_shift.dropna(inplace=True)
get_stationarity(df_log_shift)

decomposition = seasonal_decompose(df_log) 
model = ARIMA(df_log, order=(2,1,2))
results = model.fit(disp=-1)
plt.plot(df_log_shift)
plt.plot(results.fittedvalues, color='red')



df_log_shift

predictions_ARIMA_diff = pd.Series(results.fittedvalues, copy=True)
predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
predictions_ARIMA_log = pd.Series(df_log['China_Beijing'].iloc[0], index=df_log.index)
predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)
predictions_ARIMA = np.exp(predictions_ARIMA_log)
plt.plot(df)
plt.plot(predictions_ARIMA)

# results.plot_predict(1,264)

# Commented out IPython magic to ensure Python compatibility.
import rpy2
# %load_ext rpy2.ipython

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages('pracma')
# install.packages('reshape')

# Commented out IPython magic to ensure Python compatibility.
# %%R
# require(pracma)
# require(Metrics)
# require(readr)
# all<- read_csv("/content/drive/My Drive/corona-virus/ts_r.csv")
# all$X1<-NULL
# date<-all[,1]
# # add next date
# date[nrow(date) + 1,1] <-all[nrow(all),1]+1
# pred_all<-NULL
# for (n in 2:ncol(all)-1) {
#   Y<-ts(data = all[n+1], start = 1, end =nrow(all)+1)  
#   sig_w<-0.01
#   w<-sig_w*randn(1,100) # acceleration which denotes the fluctuation (Q/R) rnorm(100, mean = 0, sd = 1)
#   sig_v<-0.01
#   v<-sig_v*randn(1,100)   
#   t<-0.45
#   phi<-matrix(c(1,0,t,1),2,2)
#   gama<-matrix(c(0.5*t^2,t),2,1)
#   H<-matrix(c(1,0),1,2)
#   #Kalman
#   x0_0<-p0_0<-matrix(c(0,0),2,1)
#   p0_0<-matrix(c(1,0,0,1),2,2)
#   Q<-0.01
#   R<-0.01
#   X<-NULL
#   X2<-NULL
#   pred<-NULL
#   for (i in 0:nrow(all)) {
#     namp <-paste("p", i+1,"_",i, sep = "")
#     assign(namp, phi%*%(get(paste("p", i,"_",i, sep = "")))%*%t(phi)+gama%*%Q%*%t(gama))
#     namk <- paste("k", i+1, sep = "")
#     assign(namk,get(paste("p", i+1,"_",i, sep = ""))%*%t(H)%*%(1/(H%*%get(paste("p", i+1,"_",i, sep = ""))%*%t(H)+R)))
#     namx <- paste("x", i+1,"_",i, sep = "")
#     assign(namx,phi%*%get(paste("x", i,"_",i, sep = "")))
#     namE <- paste("E", i+1, sep = "")
#     assign(namE,Y[i+1]-H%*%get(paste("x", i+1,"_",i, sep = "")))
#     namx2 <- paste("x", i+1,"_",i+1, sep = "")
#     assign(namx2,get(paste("x", i+1,"_",i, sep = ""))+get(paste("k", i+1, sep = ""))%*%get(paste("E", i+1, sep = "")))
#     namp2 <- paste("p", i+1,"_",i+1, sep = "")
#     assign(namp2,(p0_0-get(paste("k", i+1, sep = ""))%*%H)%*%get(paste("p", i+1,"_",i, sep = "")))
#     X<-rbind(X,get(paste("x", i+1,"_",i,sep = ""))[1])
#     X2<-rbind(X2,get(paste("x", i+1,"_",i,sep = ""))[2])
#     if(i>2){
#       remove(list=(paste("p", i-1,"_",i-2, sep = "")))
#       remove(list=(paste("k", i-1, sep = "")))
#       remove(list=(paste("E", i-1, sep = "")))
#       remove(list=(paste("p", i-2,"_",i-2, sep = "")))
#       remove(list=(paste("x", i-1,"_",i-2, sep = "")))
#       remove(list=(paste("x", i-2,"_",i-2, sep = "")))}
#   }
#   pred<-NULL
#   pred<-cbind(Y,X,round(X2,4))
#   pred<-as.data.frame(pred)
#   pred$region<-colnames(all[,n+1])
#   pred$date<-date$date
#   pred$actual<-rbind(0,(cbind(pred[2:nrow(pred),1])/pred[1:nrow(pred)-1,1]-1)*100)
#   pred$predict<-rbind(0,(cbind(pred[2:nrow(pred),2])/pred[1:nrow(pred)-1,2]-1)*100)
#   pred$pred_rate<-(pred$X/pred$Y-1)*100
#   pred$X2_change<-rbind(0,(cbind(pred[2:nrow(pred),3]-pred[1:nrow(pred)-1,3])))
#   pred_all<-rbind(pred_all,pred)
# }
# pred_all<-cbind(pred_all[,4:5],pred_all[,1:3])
# names(pred_all)[5]<-"X2"
# pred_all=pred_all[with( pred_all, order(region, date)), ]
# pred_all<-pred_all[,3:5]

p=%R pred_all

############ Merge R output due to package problem
t=ts_d
t=t.stack().reset_index(name='confirmed')
t.columns=['date', 'region','confirmed']
t['date']=pd.to_datetime(t['date'] ,errors ='coerce')
t=t.sort_values(['region', 'date'])

temp=t.iloc[:,:3]
temp=temp.reset_index(drop=True)
for i in range(1,len(t)+1):
  if(temp.iloc[i,1] is not temp.iloc[i-1,1]):
    temp.loc[len(temp)+1] = [temp.iloc[i-1,0]+ pd.DateOffset(1),temp.iloc[i-1,1], 0] 
temp=temp.sort_values(['region', 'date'])
temp=temp.reset_index(drop=True)
temp['Y']=p['Y']
temp['X']=p['X']
temp['X2']=p['X2']

"""Pre Processing Data for ML Model"""

# preceding days' weather data
w=pd.read_csv('https://raw.githubusercontent.com/Rank23/COVID19/master/w.csv', sep=',', encoding='latin1')
w['date']=pd.to_datetime(w['date'],format='%d/%m/%Y')
#w['date']=pd.to_datetime(w['date'],errors ='coerce')

# weather forecast data
w_forecast=pd.read_csv('https://raw.githubusercontent.com/Rank23/COVID19/master/w.csv', sep=',', encoding='latin1')
w_forecast['date']=pd.to_datetime(w_forecast['date'],format='%d/%m/%Y')

"""Build Train Set Data Structure"""

t=ts
t=t.stack().reset_index(name='confirmed')
t.columns=['date', 'region','confirmed']
t['date']=pd.to_datetime(t['date'] ,errors ='coerce')
t=t.sort_values(['region', 'date'])

# Add 1 Future day for prediction
t=t.reset_index(drop=True)
for i in range(1,len(t)+1):
  if(t.iloc[i,1] is not t.iloc[i-1,1]):
    t.loc[len(t)+1] = [t.iloc[i-1,0]+ pd.DateOffset(1),t.iloc[i-1,1], 0] 

t=t.sort_values(['region', 'date'])
t=t.reset_index(drop=True)

print(t)

t.tail(7)

t['1_day_change']=t['3_day_change']=t['7_day_change']=t['1_day_change_rate']=t['3_day_change_rate']=t['7_day_change_rate']=t['last_day']=0
for i in range(1,len(t)):
  if(t.iloc[i,1] is t.iloc[i-2,1]):
    t.iloc[i,3]=t.iloc[i-1,2]-t.iloc[i-2,2]
    t.iloc[i,6]=(t.iloc[i-1,2]/t.iloc[i-2,2]-1)*100
    t.iloc[i,9]=t.iloc[i-1,2]
  if(t.iloc[i,1] is t.iloc[i-4,1]):
    t.iloc[i,4]=t.iloc[i-1,2]-t.iloc[i-4,2]
    t.iloc[i,7]=(t.iloc[i-1,2]/t.iloc[i-4,2]-1)*100
  if(t.iloc[i,1] is t.iloc[i-8,1]):
    t.iloc[i,5]=t.iloc[i-1,2]-t.iloc[i-8,2]
    t.iloc[i,8]=(t.iloc[i-1,2]/t.iloc[i-8,2]-1)*100
t=t.fillna(0)  
t=t.merge(temp[['date','region', 'X']],how='left',on=['date','region'])
t=t.rename(columns = {'X':'kalman_prediction'}) 
t=t.replace([np.inf, -np.inf], 0)
t['kalman_prediction']=round(t['kalman_prediction'])
train=t.merge(confirmed[['region',' Population ']],how='left',on='region')
train=train.rename(columns = {' Population ':'population'})
train['population']=train['population'].str.replace(r" ", '')
train['population']=train['population'].str.replace(r",", '')
train['population']=train['population'].fillna(1)
train['population']=train['population'].astype('int32')
train['infected_rate'] =train['last_day']/train['population']*10000
train=train.merge(w,how='left',on=['date','region'])
train=train.sort_values(['region', 'date'])
# fill missing weather 
# for i in range(0,len(train)):
#   if(np.isnan(train.iloc[i,13])):
#     if(train.iloc[i,1] is train.iloc[i-1,1]):
#       train.iloc[i,13]=train.iloc[i-1,13]
#       train.iloc[i,14]=train.iloc[i-1,14]

t.tail()

train[1500:]

# Select region
region='China_Hubei'

evaluation=pd.DataFrame(columns=['region','mse','rmse','mae'])
place=0
for i in range(1,len(t)):
  if(t.iloc[i,1] is not t.iloc[i-1,1]):
    ex=np.array(t.iloc[i-len(ts):i,10])
    pred=np.array(t.iloc[i-len(ts):i,2])    
    evaluation=evaluation.append({'region': t.iloc[i-1,1], 'mse': np.power((ex - pred),2).mean(),'rmse':sqrt(mean_squared_error(ex,pred)),'mae': (abs(ex - pred)).mean()}, ignore_index=True)
p=t[t['region']==region][['date','region','confirmed','kalman_prediction']]
p.tail(10)

p.iloc[len(p)-1,2]=None
p=p.set_index(['date'])
p.tail(10)

p.iloc[:,1:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - Select Region to Change - {}'.format(p.iloc[0,0]),fontdict={'fontsize': 22})
print(evaluation[evaluation['region']==p.iloc[0,0]])

# Select region
region='China_Shanghai'

evaluation=pd.DataFrame(columns=['region','mse','rmse','mae'])
place=0
for i in range(1,len(t)):
  if(t.iloc[i,1] is not t.iloc[i-1,1]):
    ex=np.array(t.iloc[i-len(ts):i,10])
    pred=np.array(t.iloc[i-len(ts):i,2])
    evaluation=evaluation.append({'region': t.iloc[i-1,1], 'mse': np.power((ex - pred),2).mean(),'rmse':sqrt(mean_squared_error(ex,pred)),'mae': (abs(ex - pred)).mean()}, ignore_index=True)
p=t[t['region']==region][['date','region','confirmed','kalman_prediction']]
p.iloc[len(p)-1,2]=None
p=p.set_index(['date'])
p.iloc[:,1:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - Select Region to Change - {}'.format(p.iloc[0,0]))
print(evaluation[evaluation['region']==p.iloc[0,0]])

# Select region
region='Italy_nan'

evaluation=pd.DataFrame(columns=['region','mse','rmse','mae'])
place=0
for i in range(1,len(t)):
  if(t.iloc[i,1] is not t.iloc[i-1,1]):
    ex=np.array(t.iloc[i-len(ts):i,10])
    pred=np.array(t.iloc[i-len(ts):i,2])
    evaluation=evaluation.append({'region': t.iloc[i-1,1], 'mse': np.power((ex - pred),2).mean(),'rmse':sqrt(mean_squared_error(ex,pred)),'mae': (abs(ex - pred)).mean()}, ignore_index=True)
p=t[t['region']==region][['date','region','confirmed','kalman_prediction']]
p.iloc[len(p)-1,2]=None
p=p.set_index(['date'])
p.iloc[:,1:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - {}'.format(p.iloc[0,0]),fontdict={'fontsize': 22})
print(evaluation[evaluation['region']==p.iloc[0,0]])

"""Regression - 1 Day Prediction"""

!pip install h2o
import h2o
from h2o.estimators import H2ORandomForestEstimator
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.grid.grid_search import H2OGridSearch
h2o.init(min_mem_size='8G')
import numpy as np
from sklearn.linear_model import LinearRegression

# today's date
from datetime import date, datetime, timedelta
today_int = date.today()
today_str = str(today_int)
today_str

# recent days for boosting below
d = datetime.today() - timedelta(days=5)
boosting_date = str(d.date())
boosting_date

# available datasets to train
d2 = datetime.today() - timedelta(days=1)
training_d = str(d2.date())
training_d

train.tail()

train=train.fillna(0) 
train_df=train[train['date']<training_d]

# some bootstrap to give more weight for recent days
boots=train_df[train_df['date']>=boosting_date] 
train_df=train_df.append([boots[boots['date']>=boosting_date]]*1000,ignore_index=True)
train_df_hubei=train_df[train_df['region']=='China_Hubei']
test=train[train['date']>=training_d]
test=test[test['date']<training_d]
test

x_col=[#'region',
            '1_day_change', '3_day_change','7_day_change',
            # '1_day_change_rate', 
            #'3_day_change_rate',
             '7_day_change_rate', 
            'last_day', 'kalman_prediction','infected_rate', 'min', 'max'
          ]

x=train_df[x_col]
x.tail()

y=train_df['confirmed']
y.tail()

reg = LinearRegression().fit(x,y)

# pred2=reg.predict(test[x_col])
pred2=reg.predict(x)
pred2=pd.DataFrame(pred2)
pred2=round(pred2)
pred2

len(pred2[0])
len(test['confirmed'].values)
# pred2['date']=test['date'].values
# pred2['region']=test['region'].values
# #pred2.iloc[:55]

train_h20 = h2o.H2OFrame(train_df)
train_h20_hubei = h2o.H2OFrame(train_df_hubei) # different model for Hubei
training_columns = ['region','1_day_change', '3_day_change', '7_day_change', '1_day_change_rate', '3_day_change_rate',
                    '7_day_change_rate', 'last_day', 'kalman_prediction','infected_rate', 'min', 'max'
                   ]                 
# Output parameter train against input parameters
response_column = 'confirmed'

# model = H2ORandomForestEstimator(ntrees=300, max_depth=12)
# model.train(x=training_columns, y=response_column, training_frame=train_h20)
model_hubei = H2ORandomForestEstimator(ntrees=300, max_depth=12)
model_hubei.train(x=training_columns, y=response_column, training_frame=train_h20_hubei)

test_h20 = h2o.H2OFrame(test)
# test_h20_hubei = h2o.H2OFrame(test_hubei)

model_hubei.varimp(True).iloc[:,:] # Feature importance for Hubei Model RF

"""Correlation Matrix and Temperature"""

from string import ascii_letters
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(style="white")
# Compute the correlation matrix
corr = train.iloc[:,2:].corr()
# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=np.bool))
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.9, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
print ('Correlation Matrix')

print('Correlation To Confirmed') 
print (corr.confirmed)

import matplotlib.pyplot as plt
p=train[['date','region','min','max']].set_index('date')
p=p[p['region']=='China_Hubei']
p.iloc[:,:].plot(marker='*',figsize=(12,4),color=['#19303f','#cccc00']).set_title('Daily Min/Max Temperature - Hubei',fontdict={'fontsize': 20})